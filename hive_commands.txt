<-- Creating a table and inserting data into it from a file in HDFS -->

CREATE EXTERNAL TABLE IF NOT EXISTS temp_user(
         firstname VARCHAR(64),
        lastname  VARCHAR(64),
        address   STRING,
        country   VARCHAR(64),
        city      VARCHAR(64),
        state     VARCHAR(64),
        post      STRING,
        phone1    VARCHAR(64),
        phone2    STRING,
        email     STRING,
        web       STRING
        )
        ROW FORMAT DELIMITED 
          FIELDS TERMINATED BY ','
          LINES TERMINATED BY '\n'
       STORED AS TEXTFILE;
 
LOAD DATA  INPATH '/home/user/user_table.txt' INTO TABLE temp_user;


array_type
  : ARRAY < data_type >
 
map_type
  : MAP < primitive_type, data_type >
 
struct_type
  : STRUCT < col_name : data_type [COMMENT col_comment], ...>
 
union_type
   : UNIONTYPE < data_type, data_type, ... >  -- (Note: Available in Hive 0.7.0 and later)
 
row_format
  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]
        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
        [NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)
  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]

<-- creating a table with partition and buckets-->

CREATE TABLE bucketed_user(
        firstname VARCHAR(64),
        lastname  VARCHAR(64),
        address   STRING,
        city       VARCHAR(64),
       state     VARCHAR(64),
        post      STRING,
        phone1    VARCHAR(64),
        phone2    STRING,
        email     STRING,
        web       STRING
        )
       COMMENT 'A bucketed sorted user table'
        PARTITIONED BY (country VARCHAR(64))
       CLUSTERED BY (state) SORTED BY (city) INTO 32 BUCKETS
        STORED AS SEQUENCEFILE;


<-- setting properties for creating dynamic partititon and bucketing--->


set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=1000;
set hive.enforce.bucketing = true;



<-- Inserting data into partitioned and bucketed table-->

INSERT OVERWRITE TABLE bucketed_user PARTITION (country)
       SELECT  firstname ,
               		lastname  ,
               		address   ,
               city      ,
               state     ,
               		post      ,
               		phone1    ,
               		phone2    ,
               		email     ,
               		web       ,
               		country   
        	FROM temp_user;
         
<-- CSV SERDE -->
normal CSV = 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
